{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/ajay/Downloads/mykey.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying a text string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "def sample_classify_text(text_content):\n",
    "    \"\"\"\n",
    "    Classifying Content in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze. Must include at least 20 words.\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'That actor on TV makes movies in Hollywood and also stars in a variety of popular new TV shows.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "\n",
    "    response = client.classify_text(document)\n",
    "    # Loop through classified categories returned from the API\n",
    "    for category in response.categories:\n",
    "        # Get the name of the category representing the document.\n",
    "        # See the predefined taxonomy of categories:\n",
    "        # https://cloud.google.com/natural-language/docs/categories\n",
    "        print(u\"Category name: {}\".format(category.name))\n",
    "        # Get the confidence. Number representing how certain the classifier\n",
    "        # is that this category represents the provided text.\n",
    "        print(u\"Confidence: {}\".format(category.confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Business & Industrial\n",
      "Confidence: 0.75\n",
      "Category name: /News/Business News\n",
      "Confidence: 0.6700000166893005\n"
     ]
    }
   ],
   "source": [
    "sample_classify_text(\"India's trade deficit with China fell to $48.66 billion i 2019-20 on account of decline in imports from neighbouring countries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Arts & Entertainment\n",
      "Confidence: 0.8199999928474426\n",
      "Category name: /News\n",
      "Confidence: 0.6600000262260437\n"
     ]
    }
   ],
   "source": [
    "sample_classify_text(\"West Ham rocked Chelsea's top four hopes and boosted its survival bid with a dramatic 3-2 win, while Leicester lost 2-1 at Everton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google.cloud import language\n",
    "import numpy\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, verbose=True):\n",
    "    \"\"\"Classify the input text into categories. \"\"\"\n",
    "\n",
    "    language_client = language.LanguageServiceClient()\n",
    "\n",
    "    document = language.types.Document(\n",
    "        content=text,\n",
    "        type=language.enums.Document.Type.PLAIN_TEXT)\n",
    "    response = language_client.classify_text(document)\n",
    "    categories = response.categories\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for category in categories:\n",
    "        # Turn the categories into a dictionary of the form:\n",
    "        # {category.name: category.confidence}, so that they can\n",
    "        # be treated as a sparse vector.\n",
    "        result[category.name] = category.confidence\n",
    "\n",
    "    if verbose:\n",
    "        print(text)\n",
    "        for category in categories:\n",
    "            print(u'=' * 20)\n",
    "            print(u'{:<16}: {}'.format('category', category.name))\n",
    "            print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domestic air passenger traffic is likely to witness a de-growth by 41-46 per cent during 2020-21, rating agency ICRA said. The rating agency said the domestic airlines witnessed a rather slow uptick in capacity in July 2020 despite recommencement of operations over two months ago.\n",
      "====================\n",
      "category        : /Business & Industrial\n",
      "confidence      : 0.5199999809265137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/Business & Industrial': 0.5199999809265137}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"Domestic air passenger traffic is likely to witness a de-growth by 41-46 per cent during 2020-21, rating agency ICRA said. The rating agency said the domestic airlines witnessed a rather slow uptick in capacity in July 2020 despite recommencement of operations over two months ago.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying content from local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(path, index_file):\n",
    "    \"\"\"Classify each text file in a directory and write\n",
    "    the results to the index_file.\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with io.open(file_path, 'r',encoding='utf8') as f:\n",
    "                text = f.read()\n",
    "                categories = classify(text, verbose=False)\n",
    "\n",
    "                result[filename] = categories\n",
    "        except Exception:\n",
    "            print('Failed to process {}'.format(file_path))\n",
    "\n",
    "    with io.open(index_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(result, ensure_ascii=False))\n",
    "\n",
    "    print('Texts indexed in file: {}'.format(index_file))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\cnn.pdf\n",
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\index.json\n",
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\SampleTest.pdf\n",
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\text1.txt\n",
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\text2.txt\n",
      "Failed to process C:\\Users\\ajay\\Documents\\nlp\\text3.txt\n",
      "Texts indexed in file: SampleTest.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index(\"C:\\\\Users\\\\ajay\\\\Documents\\\\nlp\", \"SampleTest.pdf\") #index(\"./2.NLP_API/\",\"SampleTest.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts indexed in file: C:\\Users\\ajay\\Documents\\nlp\\index.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text1.txt': {'/Science/Scientific Institutions': 0.7900000214576721},\n",
       " 'text2.txt': {'/Health/Health Conditions/Infectious Diseases': 0.949999988079071,\n",
       "  '/Health/Medical Facilities & Services/Medical Procedures': 0.9100000262260437,\n",
       "  '/Health/Public Health': 0.8999999761581421,\n",
       "  '/Law & Government/Public Safety': 0.8600000143051147},\n",
       " 'text3.txt': {'/Sports/Individual Sports/Racquet Sports': 0.9900000095367432,\n",
       "  '/News/Sports News': 0.5299999713897705}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index(\"C:\\\\Users\\\\ajay\\Documents\\\\nlp\",\"C:\\\\Users\\\\ajay\\Documents\\\\nlp\\\\index.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying content from Google Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "def sample_classify_text(gcs_content_uri):\n",
    "    \"\"\"\n",
    "    Classifying Content in text file stored in Cloud Storage\n",
    "\n",
    "    Args:\n",
    "      gcs_content_uri Google Cloud Storage URI where the file content is located.\n",
    "      e.g. gs://[Your Bucket]/[Path to File]\n",
    "      The text file must include at least 20 words.\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # gcs_content_uri = 'gs://cloud-samples-data/language/classify-entertainment.txt'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"gcs_content_uri\": gcs_content_uri, \"type\": type_, \"language\": language}\n",
    "\n",
    "    response = client.classify_text(document)\n",
    "    # Loop through classified categories returned from the API\n",
    "    for category in response.categories:\n",
    "        # Get the name of the category representing the document.\n",
    "        # See the predefined taxonomy of categories:\n",
    "        # https://cloud.google.com/natural-language/docs/categories\n",
    "        print(u\"Category name: {}\".format(category.name))\n",
    "        # Get the confidence. Number representing how certain the classifier\n",
    "        # is that this category represents the provided text.\n",
    "        print(u\"Confidence: {}\".format(category.confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Science/Scientific Institutions\n",
      "Confidence: 0.7900000214576721\n"
     ]
    }
   ],
   "source": [
    "sample_classify_text('gs://buck910/gcp_nlp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Arts & Entertainment\n",
      "Confidence: 0.5899999737739563\n"
     ]
    }
   ],
   "source": [
    "sample_classify_text('gs://buck910/SampleTest (1).pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying files in a directory based on a query text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(categories):\n",
    "    \"\"\"The category labels are of the form \"/a/b/c\" up to three levels,\n",
    "    for example \"/Computers & Electronics/Software\", and these labels\n",
    "    are used as keys in the categories dictionary, whose values are\n",
    "    confidence scores.\n",
    "    The split_labels function splits the keys into individual levels\n",
    "    while duplicating the confidence score, which allows a natural\n",
    "    boost in how we calculate similarity when more levels are in common.\n",
    "    Example:\n",
    "    If we have\n",
    "    x = {\"/a/b/c\": 0.5}\n",
    "    y = {\"/a/b\": 0.5}\n",
    "    z = {\"/a\": 0.5}\n",
    "    Then x and y are considered more similar than y and z.\n",
    "    \"\"\"\n",
    "    _categories = {}\n",
    "    for name, confidence in six.iteritems(categories):\n",
    "        labels = [label for label in name.split('/') if label]\n",
    "        for label in labels:\n",
    "            _categories[label] = confidence\n",
    "\n",
    "    return _categories\n",
    "\n",
    "\n",
    "def similarity(categories1, categories2):\n",
    "    \"\"\"Cosine similarity of the categories treated as sparse vectors.\"\"\"\n",
    "    categories1 = split_labels(categories1)\n",
    "    categories2 = split_labels(categories2)\n",
    "\n",
    "    norm1 = numpy.linalg.norm(list(categories1.values()))\n",
    "    norm2 = numpy.linalg.norm(list(categories2.values()))\n",
    "\n",
    "    # Return the smallest possible similarity if either categories is empty.\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute the cosine similarity.\n",
    "    dot = 0.0\n",
    "    for label, confidence in six.iteritems(categories1):\n",
    "        dot += confidence * categories2.get(label, 0.0)\n",
    "\n",
    "    return dot / (norm1 * norm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_category(index_file , category_string, n_top=3):\n",
    "    \"\"\"Find the indexed files that are the most similar to\n",
    "    the query label.\n",
    "\n",
    "    The list of all available labels:\n",
    "    https://cloud.google.com/natural-language/docs/categories\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(index_file, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    # Make the category_string into a dictionary so that it is\n",
    "    # of the same format as what we get by calling classify.\n",
    "    query_categories = {category_string: 1.0}\n",
    "\n",
    "    similarities = []\n",
    "    for filename, categories in six.iteritems(index):\n",
    "        similarities.append(\n",
    "            (filename, similarity(query_categories, categories)))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda p: p[1], reverse=True)\n",
    "\n",
    "    print('=' * 20)\n",
    "    print('Query: {}\\n'.format(category_string))\n",
    "    print('\\nMost similar {} indexed texts:'.format(n_top))\n",
    "    for filename, sim in similarities[:n_top]:\n",
    "        print('\\tFilename: {}'.format(filename))\n",
    "        print('\\tSimilarity: {}'.format(sim))\n",
    "        print('\\n')\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Query: /Science/Scientific Institutions\n",
      "\n",
      "\n",
      "Most similar 3 indexed texts:\n",
      "\tFilename: text1.txt\n",
      "\tSimilarity: 0.9999999999999999\n",
      "\n",
      "\n",
      "\tFilename: text2.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n",
      "\tFilename: text3.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text1.txt', 0.9999999999999999), ('text2.txt', 0.0), ('text3.txt', 0.0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_category(index_file = \"C:/Users/ajay/Documents/nlp/index.json\",category_string = '/Science/Scientific Institutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Query: /News/Sports News\n",
      "\n",
      "\n",
      "Most similar 3 indexed texts:\n",
      "\tFilename: text3.txt\n",
      "\tSimilarity: 0.40052218135527856\n",
      "\n",
      "\n",
      "\tFilename: text1.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n",
      "\tFilename: text2.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text3.txt', 0.40052218135527856), ('text1.txt', 0.0), ('text2.txt', 0.0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_category(index_file = \"C:/Users/ajay/Documents/nlp/index.json\",category_string = '/News/Sports News')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying files in a directory based on a query label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(index_file, text, n_top=3):\n",
    "    \"\"\"Find the indexed files that are the most similar to\n",
    "    the query text.\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(index_file, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    # Get the categories of the query text.\n",
    "    query_categories = classify(text, verbose=False)\n",
    "\n",
    "    similarities = []\n",
    "    for filename, categories in six.iteritems(index):\n",
    "        similarities.append(\n",
    "            (filename, similarity(query_categories, categories)))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda p: p[1], reverse=True)\n",
    "\n",
    "    print('=' * 20)\n",
    "    print('Query: {}\\n'.format(text))\n",
    "    for category, confidence in six.iteritems(query_categories):\n",
    "        print('\\tCategory: {}, confidence: {}'.format(category, confidence))\n",
    "    print('\\nMost similar {} indexed texts:'.format(n_top))\n",
    "    for filename, sim in similarities[:n_top]:\n",
    "        print('\\tFilename: {}'.format(filename))\n",
    "        print('\\tSimilarity: {}'.format(sim))\n",
    "        print('\\n')\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Query: The two longest works that scholars agree were written by Shakespeare are entitled Venus and Adonis and The Rape of Lucrece. Both dedicated to the Honorable Henry Wriothesley, Earl of Southampton, who seems to have acted as a sponsor and encouraging benefactor of Shakespeare's work for a brief time.\n",
      "\n",
      "\n",
      "Most similar 3 indexed texts:\n",
      "\tFilename: text1.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n",
      "\tFilename: text2.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n",
      "\tFilename: text3.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text1.txt', 0.0), ('text2.txt', 0.0), ('text3.txt', 0.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(index_file = \"C:/Users/ajay/Documents/nlp/index.json\",text =\"The two longest works that scholars agree were written by Shakespeare are entitled Venus and Adonis and The Rape of Lucrece. Both dedicated to the Honorable Henry Wriothesley, Earl of Southampton, who seems to have acted as a sponsor and encouraging benefactor of Shakespeare's work for a brief time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Query: The virus spreads easily and the majority of the world's population is still vulnerable to it. A vaccine would provide some protection by training people's immune systems to fight the virus so they should not become sick.\n",
      "\n",
      "\tCategory: /Health/Health Conditions/Infectious Diseases, confidence: 0.6200000047683716\n",
      "\tCategory: /Law & Government/Public Safety, confidence: 0.5\n",
      "\n",
      "Most similar 3 indexed texts:\n",
      "\tFilename: text2.txt\n",
      "\tSimilarity: 0.7882733184241837\n",
      "\n",
      "\n",
      "\tFilename: text1.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n",
      "\tFilename: text3.txt\n",
      "\tSimilarity: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text2.txt', 0.7882733184241837), ('text1.txt', 0.0), ('text3.txt', 0.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(index_file =\"C:/Users/ajay/Documents/nlp/index.json\", text=\"The virus spreads easily and the majority of the world's population is still vulnerable to it. A vaccine would provide some protection by training people's immune systems to fight the virus so they should not become sick.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
