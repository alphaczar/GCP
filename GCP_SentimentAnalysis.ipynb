{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/ajay/Downloads/mykey.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentiment of a text passed by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Tomorrow is going to be the end of the world\n",
      "Sentiment: -0.10000000149011612, 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# The text to analyze\n",
    "text = u'Tomorrow is going to be the end of the world'\n",
    "document = types.Document(\n",
    "    content=text,\n",
    "    type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "print('Text: {}'.format(text))\n",
    "print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "\n",
    "def sample_analyze_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'I am so happy and joyful.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.800000011920929\n",
      "Document sentiment magnitude: 0.800000011920929\n",
      "Sentence text: West Ham rocked Chelsea's top four hopes and boosted its survival bid with a dramatic 3-2 win, while Leicester lost 2-1 at Everton\n",
      "Sentence sentiment score: 0.800000011920929\n",
      "Sentence sentiment magnitude: 0.800000011920929\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "sample_analyze_sentiment(\"West Ham rocked Chelsea's top four hopes and boosted its survival bid with a dramatic 3-2 win, while Leicester lost 2-1 at Everton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentiment of a text in a picture (need to use Vision API also here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(path):\n",
    "    \"\"\"Detects text in the file.\"\"\"\n",
    "    from google.cloud import vision\n",
    "    import io\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    print('Texts:')\n",
    "\n",
    "    for text in texts:\n",
    "        print('\\n\"{}\"'.format(text.description))\n",
    "\n",
    "        vertices = (['({},{})'.format(vertex.x, vertex.y)\n",
    "                    for vertex in text.bounding_poly.vertices])\n",
    "\n",
    "        print('bounds: {}'.format(','.join(vertices)))\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      "\n",
      "\"AllBusiness\n",
      "Templates\n",
      "PERMISSION TO AVAIL LEAVE\n",
      "Date: November 28, 2017\n",
      "To,\n",
      "ATTN\n",
      "ADDRESS\n",
      "POSTAL CODE\n",
      "COUNTRY\n",
      "Subject: permission to avail leave\n",
      "Dear Mr./Mrs.,\n",
      "I am writing this letter to ask your permission to avail annual leave for . Days (mention duration)\n",
      "from . to .(mention dates) due to .(state reasons in brief). I will report back for work on\n",
      ".(mention date of joining back).\n",
      "I will hand over my duties and responsibilities to my colleague and team-mate, .(mention colleague's\n",
      "name) and brief him about the works in hand.\n",
      "I will be obliged if you approve my leave and get in touch in case I require any further extension of\n",
      "leave. I will leave my contact details with your office in case of any queries.\n",
      "Thanking You.\n",
      "If you have any questions, please feel free to contact me directly. Looking forward to your reply.\n",
      "Yours sincerely,\n",
      "<NAME>\n",
      "<Signature>\n",
      "<Formal Name + Title>\n",
      "Page 1 of 2\n",
      "\"\n",
      "bounds: (94,69),(739,69),(739,1074),(94,1074)\n",
      "\n",
      "\"AllBusiness\"\n",
      "bounds: (690,69),(739,70),(739,77),(690,76)\n",
      "\n",
      "\"Templates\"\n",
      "bounds: (692,80),(737,80),(737,88),(692,88)\n",
      "\n",
      "\"PERMISSION\"\n",
      "bounds: (272,102),(378,102),(378,114),(272,114)\n",
      "\n",
      "\"TO\"\n",
      "bounds: (386,102),(407,102),(407,114),(386,114)\n",
      "\n",
      "\"AVAIL\"\n",
      "bounds: (414,102),(464,102),(464,114),(414,114)\n",
      "\n",
      "\"LEAVE\"\n",
      "bounds: (472,102),(521,102),(521,114),(472,114)\n",
      "\n",
      "\"Date:\"\n",
      "bounds: (95,138),(124,138),(124,146),(95,146)\n",
      "\n",
      "\"November\"\n",
      "bounds: (131,137),(195,137),(195,146),(131,146)\n",
      "\n",
      "\"28,\"\n",
      "bounds: (200,138),(214,138),(214,147),(200,147)\n",
      "\n",
      "\"2017\"\n",
      "bounds: (221,138),(246,138),(246,146),(221,146)\n",
      "\n",
      "\"To,\"\n",
      "bounds: (94,198),(109,198),(109,208),(94,208)\n",
      "\n",
      "\"ATTN\"\n",
      "bounds: (94,216),(123,216),(123,225),(94,225)\n",
      "\n",
      "\"ADDRESS\"\n",
      "bounds: (95,233),(148,234),(148,243),(95,242)\n",
      "\n",
      "\"POSTAL\"\n",
      "bounds: (95,252),(139,252),(139,260),(95,260)\n",
      "\n",
      "\"CODE\"\n",
      "bounds: (143,251),(174,251),(174,260),(143,260)\n",
      "\n",
      "\"COUNTRY\"\n",
      "bounds: (95,270),(151,270),(151,278),(95,278)\n",
      "\n",
      "\"Subject:\"\n",
      "bounds: (94,340),(141,340),(141,352),(94,352)\n",
      "\n",
      "\"permission\"\n",
      "bounds: (148,341),(214,340),(214,350),(148,351)\n",
      "\n",
      "\"to\"\n",
      "bounds: (219,341),(230,341),(230,349),(219,349)\n",
      "\n",
      "\"avail\"\n",
      "bounds: (235,340),(258,340),(258,350),(235,350)\n",
      "\n",
      "\"leave\"\n",
      "bounds: (264,340),(291,340),(291,349),(264,349)\n",
      "\n",
      "\"Dear\"\n",
      "bounds: (95,395),(122,395),(122,403),(95,403)\n",
      "\n",
      "\"Mr./Mrs.,\"\n",
      "bounds: (127,393),(181,393),(181,406),(127,406)\n",
      "\n",
      "\"I\"\n",
      "bounds: (101,445),(110,445),(110,459),(101,459)\n",
      "\n",
      "\"am\"\n",
      "bounds: (112,445),(125,445),(125,459),(112,459)\n",
      "\n",
      "\"writing\"\n",
      "bounds: (127,448),(168,448),(168,459),(127,459)\n",
      "\n",
      "\"this\"\n",
      "bounds: (173,447),(193,447),(193,456),(173,456)\n",
      "\n",
      "\"letter\"\n",
      "bounds: (201,447),(231,447),(231,456),(201,456)\n",
      "\n",
      "\"to\"\n",
      "bounds: (237,448),(248,448),(248,456),(237,456)\n",
      "\n",
      "\"ask\"\n",
      "bounds: (255,447),(271,447),(271,456),(255,456)\n",
      "\n",
      "\"your\"\n",
      "bounds: (279,450),(305,450),(305,459),(279,459)\n",
      "\n",
      "\"permission\"\n",
      "bounds: (312,448),(374,448),(374,459),(312,459)\n",
      "\n",
      "\"to\"\n",
      "bounds: (381,448),(392,448),(392,456),(381,456)\n",
      "\n",
      "\"avail\"\n",
      "bounds: (399,447),(422,447),(422,456),(399,456)\n",
      "\n",
      "\"annual\"\n",
      "bounds: (431,447),(468,447),(468,456),(431,456)\n",
      "\n",
      "\"leave\"\n",
      "bounds: (477,447),(505,447),(505,456),(477,456)\n",
      "\n",
      "\"for\"\n",
      "bounds: (512,447),(528,447),(528,456),(512,456)\n",
      "\n",
      "\".\"\n",
      "bounds: (535,455),(546,455),(546,456),(535,456)\n",
      "\n",
      "\"Days\"\n",
      "bounds: (553,448),(578,448),(578,459),(553,459)\n",
      "\n",
      "\"(mention\"\n",
      "bounds: (586,447),(638,447),(638,459),(586,459)\n",
      "\n",
      "\"duration)\"\n",
      "bounds: (645,447),(697,447),(697,459),(645,459)\n",
      "\n",
      "\"from\"\n",
      "bounds: (94,467),(121,467),(121,476),(94,476)\n",
      "\n",
      "\".\"\n",
      "bounds: (131,475),(142,475),(142,476),(131,476)\n",
      "\n",
      "\"to\"\n",
      "bounds: (151,468),(162,468),(162,476),(151,476)\n",
      "\n",
      "\".(mention\"\n",
      "bounds: (172,467),(237,467),(237,478),(172,478)\n",
      "\n",
      "\"dates)\"\n",
      "bounds: (247,467),(281,467),(281,478),(247,478)\n",
      "\n",
      "\"due\"\n",
      "bounds: (291,467),(311,467),(311,476),(291,476)\n",
      "\n",
      "\"to\"\n",
      "bounds: (320,468),(331,468),(331,476),(320,476)\n",
      "\n",
      "\".(state\"\n",
      "bounds: (341,467),(382,467),(382,478),(341,478)\n",
      "\n",
      "\"reasons\"\n",
      "bounds: (392,470),(435,470),(435,476),(392,476)\n",
      "\n",
      "\"in\"\n",
      "bounds: (445,468),(453,468),(453,476),(445,476)\n",
      "\n",
      "\"brief).\"\n",
      "bounds: (463,467),(496,467),(496,478),(463,478)\n",
      "\n",
      "\"I\"\n",
      "bounds: (505,464),(516,464),(516,478),(505,478)\n",
      "\n",
      "\"will\"\n",
      "bounds: (516,467),(534,467),(534,476),(516,476)\n",
      "\n",
      "\"report\"\n",
      "bounds: (545,468),(581,468),(581,478),(545,478)\n",
      "\n",
      "\"back\"\n",
      "bounds: (590,467),(615,467),(615,476),(590,476)\n",
      "\n",
      "\"for\"\n",
      "bounds: (623,467),(639,467),(639,476),(623,476)\n",
      "\n",
      "\"work\"\n",
      "bounds: (647,467),(675,467),(675,476),(647,476)\n",
      "\n",
      "\"on\"\n",
      "bounds: (685,470),(697,470),(697,476),(685,476)\n",
      "\n",
      "\".(mention\"\n",
      "bounds: (95,486),(163,486),(163,498),(95,498)\n",
      "\n",
      "\"date\"\n",
      "bounds: (168,486),(193,486),(193,495),(168,495)\n",
      "\n",
      "\"of\"\n",
      "bounds: (197,483),(210,483),(210,500),(197,500)\n",
      "\n",
      "\"joining\"\n",
      "bounds: (212,483),(255,483),(255,500),(212,500)\n",
      "\n",
      "\"back).\"\n",
      "bounds: (260,486),(290,486),(290,498),(260,498)\n",
      "\n",
      "\"I\"\n",
      "bounds: (101,513),(109,513),(109,529),(101,529)\n",
      "\n",
      "\"will\"\n",
      "bounds: (111,513),(129,513),(129,529),(111,529)\n",
      "\n",
      "\"hand\"\n",
      "bounds: (131,513),(154,513),(154,529),(131,529)\n",
      "\n",
      "\"over\"\n",
      "bounds: (160,519),(185,519),(185,525),(160,525)\n",
      "\n",
      "\"my\"\n",
      "bounds: (190,519),(205,519),(205,528),(190,528)\n",
      "\n",
      "\"duties\"\n",
      "bounds: (212,516),(246,516),(246,525),(212,525)\n",
      "\n",
      "\"and\"\n",
      "bounds: (252,516),(271,516),(271,525),(252,525)\n",
      "\n",
      "\"responsibilities\"\n",
      "bounds: (278,516),(364,516),(364,528),(278,528)\n",
      "\n",
      "\"to\"\n",
      "bounds: (370,517),(381,517),(381,525),(370,525)\n",
      "\n",
      "\"my\"\n",
      "bounds: (387,519),(402,519),(402,528),(387,528)\n",
      "\n",
      "\"colleague\"\n",
      "bounds: (408,516),(462,516),(462,528),(408,528)\n",
      "\n",
      "\"and\"\n",
      "bounds: (468,516),(487,516),(487,525),(468,525)\n",
      "\n",
      "\"team-mate,\"\n",
      "bounds: (493,517),(561,517),(561,527),(493,527)\n",
      "\n",
      "\".(mention\"\n",
      "bounds: (567,516),(628,516),(628,528),(567,528)\n",
      "\n",
      "\"colleague's\"\n",
      "bounds: (635,516),(697,516),(697,528),(635,528)\n",
      "\n",
      "\"name)\"\n",
      "bounds: (95,535),(132,535),(132,546),(95,546)\n",
      "\n",
      "\"and\"\n",
      "bounds: (137,535),(157,535),(157,544),(137,544)\n",
      "\n",
      "\"brief\"\n",
      "bounds: (163,535),(191,535),(191,544),(163,544)\n",
      "\n",
      "\"him\"\n",
      "bounds: (195,535),(216,535),(216,544),(195,544)\n",
      "\n",
      "\"about\"\n",
      "bounds: (221,535),(255,535),(255,544),(221,544)\n",
      "\n",
      "\"the\"\n",
      "bounds: (259,535),(278,535),(278,544),(259,544)\n",
      "\n",
      "\"works\"\n",
      "bounds: (283,535),(318,535),(318,544),(283,544)\n",
      "\n",
      "\"in\"\n",
      "bounds: (323,536),(332,536),(332,544),(323,544)\n",
      "\n",
      "\"hand.\"\n",
      "bounds: (337,535),(365,535),(365,544),(337,544)\n",
      "\n",
      "\"I\"\n",
      "bounds: (106,563),(112,563),(112,577),(106,577)\n",
      "\n",
      "\"will\"\n",
      "bounds: (114,563),(130,563),(130,577),(114,577)\n",
      "\n",
      "\"be\"\n",
      "bounds: (129,565),(141,565),(141,574),(129,574)\n",
      "\n",
      "\"obliged\"\n",
      "bounds: (148,565),(189,565),(189,577),(148,577)\n",
      "\n",
      "\"if\"\n",
      "bounds: (197,565),(203,565),(203,574),(197,574)\n",
      "\n",
      "\"you\"\n",
      "bounds: (208,568),(228,568),(228,577),(208,577)\n",
      "\n",
      "\"approve\"\n",
      "bounds: (235,568),(281,568),(281,577),(235,577)\n",
      "\n",
      "\"my\"\n",
      "bounds: (289,568),(305,568),(305,577),(289,577)\n",
      "\n",
      "\"leave\"\n",
      "bounds: (311,565),(339,565),(339,574),(311,574)\n",
      "\n",
      "\"and\"\n",
      "bounds: (347,565),(366,565),(366,574),(347,574)\n",
      "\n",
      "\"get\"\n",
      "bounds: (373,566),(391,566),(391,577),(373,577)\n",
      "\n",
      "\"in\"\n",
      "bounds: (397,566),(405,566),(405,574),(397,574)\n",
      "\n",
      "\"touch\"\n",
      "bounds: (412,565),(444,565),(444,574),(412,574)\n",
      "\n",
      "\"in\"\n",
      "bounds: (452,566),(460,566),(460,574),(452,574)\n",
      "\n",
      "\"case\"\n",
      "bounds: (467,568),(490,568),(490,574),(467,574)\n",
      "\n",
      "\"I\"\n",
      "bounds: (497,563),(507,563),(507,577),(497,577)\n",
      "\n",
      "\"require\"\n",
      "bounds: (509,563),(547,563),(547,577),(509,577)\n",
      "\n",
      "\"any\"\n",
      "bounds: (554,568),(573,568),(573,577),(554,577)\n",
      "\n",
      "\"further\"\n",
      "bounds: (578,565),(619,565),(619,574),(578,574)\n",
      "\n",
      "\"extension\"\n",
      "bounds: (626,566),(681,566),(681,574),(626,574)\n",
      "\n",
      "\"of\"\n",
      "bounds: (688,565),(698,565),(698,574),(688,574)\n",
      "\n",
      "\"leave.\"\n",
      "bounds: (95,585),(130,585),(130,594),(95,594)\n",
      "\n",
      "\"I\"\n",
      "bounds: (135,582),(145,582),(145,595),(135,595)\n",
      "\n",
      "\"will\"\n",
      "bounds: (142,585),(161,585),(161,594),(142,594)\n",
      "\n",
      "\"leave\"\n",
      "bounds: (167,585),(198,585),(198,594),(167,594)\n",
      "\n",
      "\"my\"\n",
      "bounds: (204,587),(220,587),(220,595),(204,595)\n",
      "\n",
      "\"contact\"\n",
      "bounds: (225,586),(268,586),(268,595),(225,595)\n",
      "\n",
      "\"details\"\n",
      "bounds: (272,585),(312,585),(312,594),(272,594)\n",
      "\n",
      "\"with\"\n",
      "bounds: (317,585),(342,585),(342,594),(317,594)\n",
      "\n",
      "\"your\"\n",
      "bounds: (347,588),(373,588),(373,596),(347,596)\n",
      "\n",
      "\"office\"\n",
      "bounds: (378,585),(409,585),(409,594),(378,594)\n",
      "\n",
      "\"in\"\n",
      "bounds: (414,586),(423,586),(423,594),(414,594)\n",
      "\n",
      "\"case\"\n",
      "bounds: (428,587),(452,587),(452,594),(428,594)\n",
      "\n",
      "\"of\"\n",
      "bounds: (457,584),(467,584),(467,594),(457,594)\n",
      "\n",
      "\"any\"\n",
      "bounds: (471,587),(489,587),(489,596),(471,596)\n",
      "\n",
      "\"queries.\"\n",
      "bounds: (494,585),(535,585),(535,597),(494,597)\n",
      "\n",
      "\"Thanking\"\n",
      "bounds: (94,615),(149,615),(149,627),(94,627)\n",
      "\n",
      "\"You.\"\n",
      "bounds: (153,616),(174,616),(174,624),(153,624)\n",
      "\n",
      "\"If\"\n",
      "bounds: (95,645),(102,645),(102,654),(95,654)\n",
      "\n",
      "\"you\"\n",
      "bounds: (106,648),(126,648),(126,657),(106,657)\n",
      "\n",
      "\"have\"\n",
      "bounds: (132,645),(158,645),(158,654),(132,654)\n",
      "\n",
      "\"any\"\n",
      "bounds: (164,648),(184,648),(184,657),(164,657)\n",
      "\n",
      "\"questions,\"\n",
      "bounds: (188,646),(249,646),(249,657),(188,657)\n",
      "\n",
      "\"please\"\n",
      "bounds: (255,645),(288,645),(288,657),(255,657)\n",
      "\n",
      "\"feel\"\n",
      "bounds: (294,645),(316,645),(316,654),(294,654)\n",
      "\n",
      "\"free\"\n",
      "bounds: (322,645),(342,645),(342,654),(322,654)\n",
      "\n",
      "\"to\"\n",
      "bounds: (347,646),(357,646),(357,654),(347,654)\n",
      "\n",
      "\"contact\"\n",
      "bounds: (363,646),(406,646),(406,654),(363,654)\n",
      "\n",
      "\"me\"\n",
      "bounds: (410,648),(427,648),(427,654),(410,654)\n",
      "\n",
      "\"directly.\"\n",
      "bounds: (433,645),(477,645),(477,657),(433,657)\n",
      "\n",
      "\"Looking\"\n",
      "bounds: (483,645),(529,645),(529,657),(483,657)\n",
      "\n",
      "\"forward\"\n",
      "bounds: (533,645),(580,645),(580,654),(533,654)\n",
      "\n",
      "\"to\"\n",
      "bounds: (584,646),(596,646),(596,654),(584,654)\n",
      "\n",
      "\"your\"\n",
      "bounds: (600,648),(626,648),(626,657),(600,657)\n",
      "\n",
      "\"reply.\"\n",
      "bounds: (631,645),(659,645),(659,657),(631,657)\n",
      "\n",
      "\"Yours\"\n",
      "bounds: (94,676),(125,676),(125,685),(94,685)\n",
      "\n",
      "\"sincerely,\"\n",
      "bounds: (130,675),(182,675),(182,688),(130,688)\n",
      "\n",
      "\"<NAME>\"\n",
      "bounds: (94,711),(145,711),(145,719),(94,719)\n",
      "\n",
      "\"<Signature>\"\n",
      "bounds: (94,747),(164,747),(164,758),(94,758)\n",
      "\n",
      "\"<Formal\"\n",
      "bounds: (94,799),(141,799),(141,809),(94,809)\n",
      "\n",
      "\"Name\"\n",
      "bounds: (148,800),(181,800),(181,810),(148,810)\n",
      "\n",
      "\"+\"\n",
      "bounds: (186,802),(192,802),(192,808),(186,808)\n",
      "\n",
      "\"Title>\"\n",
      "bounds: (195,800),(228,800),(228,809),(195,809)\n",
      "\n",
      "\"Page\"\n",
      "bounds: (369,1065),(387,1065),(387,1074),(369,1074)\n",
      "\n",
      "\"1\"\n",
      "bounds: (392,1065),(395,1065),(395,1072),(392,1072)\n",
      "\n",
      "\"of\"\n",
      "bounds: (399,1064),(407,1064),(407,1071),(399,1071)\n",
      "\n",
      "\"2\"\n",
      "bounds: (410,1065),(414,1065),(414,1071),(410,1071)\n"
     ]
    }
   ],
   "source": [
    "detect_text(\"C:/Users/ajay/Documents/leave letter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE END\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import vision\n",
    "import io\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "with io.open(\"C:/Users/ajay/Documents/nlp/img3.jpg\", 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.types.Image(content=content)\n",
    "response = client.text_detection(image=image)\n",
    "\n",
    "texts = response.text_annotations\n",
    "texts1 = [text.description for text in response.text_annotations]\n",
    "test=texts1[0]\n",
    "test = test.replace('\\n','')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: THE END\n",
      "Sentiment: 0.30000001192092896, 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# The text to analyze\n",
    "text = test\n",
    "document = types.Document(\n",
    "    content=text,\n",
    "    type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "print('Text: {}'.format(text))\n",
    "print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "import io\n",
    "\n",
    "def extract_text_fromimage(imagepath):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    with io.open(imagepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "        \n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    texts1 = [text.description for text in response.text_annotations]\n",
    "    test=texts1[0]\n",
    "    test = test.replace('\\n','')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of text from Google Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "\n",
    "def sample_analyze_sentiment(gcs_content_uri):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in text file stored in Cloud Storage\n",
    "\n",
    "    Args:\n",
    "      gcs_content_uri Google Cloud Storage URI where the file content is located.\n",
    "      e.g. gs://[Your Bucket]/[Path to File]\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # gcs_content_uri = 'gs://cloud-samples-data/language/sentiment-positive.txt'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"gcs_content_uri\": gcs_content_uri, \"type\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.0\n",
      "Document sentiment magnitude: 1.100000023841858\n",
      "Sentence text: The move by The American Association for the Advancement of Science (AAAS), publisher of the esteemed Science journals, comes after Black scientists came forward to protest racism within academia and the sciences, and organized a strike on June 10th, that AAAS joined.\n",
      "Sentence sentiment score: -0.5\n",
      "Sentence sentiment magnitude: 0.5\n",
      "Sentence text: In a letter to its 120,000 members this month, AAAS CEO Sudip Parikh announced that the 172-year-old institution has come up with a plan to hold itself accountable for making itself and the sciences more diverse.\n",
      "Sentence sentiment score: 0.30000001192092896\n",
      "Sentence sentiment magnitude: 0.30000001192092896\n",
      "Sentence text: By September, it will start sharing data on the diversity of its staff, fellows, and authors published in its journals.\n",
      "Sentence sentiment score: 0.20000000298023224\n",
      "Sentence sentiment magnitude: 0.20000000298023224\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "sample_analyze_sentiment('gs://buck910/gcp_nlp.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of image from Google Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_fromGSimage(GSimage_uri):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    image = vision.types.Image()\n",
    "    image.source.image_uri = GSimage_uri\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    texts1 = [text.description for text in response.text_annotations]\n",
    "    test=texts1[0]\n",
    "    test = test.replace('\\n','')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown foxjumped over the 5lazy dogs!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_fromGSimage(GSimage_uri = \"gs://buck910/ocr2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: THE END\n",
      "Sentiment: 0.30000001192092896, 0.30000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()\n",
    "\n",
    "# The text to analyze\n",
    "text = test\n",
    "document = types.Document(\n",
    "    content=text,\n",
    "    type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "# Detects the sentiment of the text\n",
    "sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "\n",
    "print('Text: {}'.format(text))\n",
    "print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extracting Sentiment from a  PDF in google storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent_PDF(gcs_source_uri, gcs_destination_uri):\n",
    "    \"\"\"OCR with PDF/TIFF as source files on GCS\"\"\"\n",
    "\n",
    "    import re\n",
    "    from google.cloud import vision\n",
    "    from google.cloud import storage\n",
    "    from google.protobuf import json_format\n",
    "    # Supported mime_types are: 'application/pdf' and 'image/tiff'\n",
    "    mime_type = 'application/pdf'\n",
    "\n",
    "    # How many pages should be grouped into each json output file.\n",
    "    batch_size = 1\n",
    "\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    feature = vision.types.Feature(\n",
    "        type=vision.enums.Feature.Type.DOCUMENT_TEXT_DETECTION)\n",
    "\n",
    "    gcs_source = vision.types.GcsSource(uri='gs://buck910/Review1.pdf')\n",
    "    input_config = vision.types.InputConfig(\n",
    "        gcs_source=gcs_source, mime_type=mime_type)\n",
    "\n",
    "    gcs_destination = vision.types.GcsDestination(uri=gcs_destination_uri)\n",
    "    output_config = vision.types.OutputConfig(\n",
    "        gcs_destination=gcs_destination, batch_size=batch_size)\n",
    "\n",
    "    async_request = vision.types.AsyncAnnotateFileRequest(\n",
    "        features=[feature], input_config=input_config,\n",
    "        output_config=output_config)\n",
    "\n",
    "    operation = client.async_batch_annotate_files(\n",
    "        requests=[async_request])\n",
    "\n",
    "    print('Waiting for the operation to finish.')\n",
    "    operation.result(timeout=420)\n",
    "\n",
    "    # Once the request has completed and the output has been\n",
    "    # written to GCS, we can list all the output files.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    match = re.match(r'gs://([^/]+)/(.+)', gcs_destination_uri)\n",
    "    bucket_name = match.group(1)\n",
    "    prefix = match.group(2)\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # List objects with the given prefix.\n",
    "    blob_list = list(bucket.list_blobs(prefix=prefix))\n",
    "    print('Output files:')\n",
    "    for blob in blob_list:\n",
    "        print(blob.name)\n",
    "\n",
    "    # Process the first output file from GCS.\n",
    "    # Since we specified batch_size=2, the first response contains\n",
    "    # the first two pages of the input file.\n",
    "    output = blob_list[0]\n",
    "\n",
    "    json_string = output.download_as_string()\n",
    "    response = json_format.Parse(\n",
    "        json_string, vision.types.AnnotateFileResponse())\n",
    "\n",
    "    # The actual response for the first page of the input file.\n",
    "    first_page_response = response.responses[0]\n",
    "    annotation = first_page_response.full_text_annotation\n",
    "\n",
    "    # Here we print the full text from the first page.\n",
    "    # The response contains more information:\n",
    "    # annotation/pages/blocks/paragraphs/words/symbols\n",
    "    # including confidence scores and bounding boxes\n",
    "    print(u'Full text:\\n{}'.format(\n",
    "        annotation.text))\n",
    "    sent_text = annotation.text\n",
    "    sample_analyze_sentiment(sent_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for the operation to finish.\n",
      "Output files:\n",
      "Review1_out.jsonoutput-1-to-1.json\n",
      "Full text:\n",
      "Cons\n",
      "I would say the biggest issue that I have simply revolves around how much we've\n",
      "grown in such a short amount of time. Being a new employee in Professional Services\n",
      "has been tough and would have been impossible without my amazing team. There's so\n",
      "MUCH information out there in various tools that there's just no way a person can\n",
      "understand where to go and possibly consume it all. A lot of relevant information is\n",
      "buried under irrelevant information. New tools come out, are exciting for a bit, then die\n",
      "away. The second largest issue in my mind is that the goals and desires of the\n",
      "Professional Services organization do not jive AT ALL with the goals and desires of my\n",
      "particular client. Now, I know that I have tunnel vision and that I haven't had enough\n",
      "experience with any other clients yet - I'm sure that with more time and exposure,\n",
      "more insights will occur. But I really feel like the work that I'm doing for my client has\n",
      "no relation to the goals of the overall organization. Here's the thing though - I know\n",
      "that improvement to these areas are in the works. It takes time to think through the\n",
      "best ways to deal with these issues and I have faith that the time will come where\n",
      "these cons won't be listed here anymore and new cons will take their place.\n",
      "Show Less\n",
      "Advice to Management\n",
      "Please continue to be transparent. Please continue to help us make organization\n",
      "changes that lead to better outcomes for our clients and ourselves. But mostly, please\n",
      "continue to invest in us the employees. I've watched these folks make miracles happen\n",
      "for you to help you achieve your goals and dreams. Putting your employees first is what\n",
      "allows us to be great - I sincerely and deeply hope that will always continue going\n",
      "forward.\n",
      "\n",
      "Document sentiment score: 0.0\n",
      "Document sentiment magnitude: 7.599999904632568\n",
      "Sentence text: Cons\n",
      "I would say the biggest issue that I have simply revolves around how much we've\n",
      "grown in such a short amount of time.\n",
      "Sentence sentiment score: -0.6000000238418579\n",
      "Sentence sentiment magnitude: 0.6000000238418579\n",
      "Sentence text: Being a new employee in Professional Services\n",
      "has been tough and would have been impossible without my amazing team.\n",
      "Sentence sentiment score: 0.8999999761581421\n",
      "Sentence sentiment magnitude: 0.8999999761581421\n",
      "Sentence text: There's so\n",
      "MUCH information out there in various tools that there's just no way a person can\n",
      "understand where to go and possibly consume it all.\n",
      "Sentence sentiment score: -0.4000000059604645\n",
      "Sentence sentiment magnitude: 0.4000000059604645\n",
      "Sentence text: A lot of relevant information is\n",
      "buried under irrelevant information.\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: New tools come out, are exciting for a bit, then die\n",
      "away.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: The second largest issue in my mind is that the goals and desires of the\n",
      "Professional Services organization do not jive AT ALL with the goals and desires of my\n",
      "particular client.\n",
      "Sentence sentiment score: -0.699999988079071\n",
      "Sentence sentiment magnitude: 0.699999988079071\n",
      "Sentence text: Now, I know that I have tunnel vision and that I haven't had enough\n",
      "experience with any other clients yet - I'm sure that with more time and exposure,\n",
      "more insights will occur.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: But I really feel like the work that I'm doing for my client has\n",
      "no relation to the goals of the overall organization.\n",
      "Sentence sentiment score: -0.8999999761581421\n",
      "Sentence sentiment magnitude: 0.8999999761581421\n",
      "Sentence text: Here's the thing though - I know\n",
      "that improvement to these areas are in the works.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: It takes time to think through the\n",
      "best ways to deal with these issues and I have faith that the time will come where\n",
      "these cons won't be listed here anymore and new cons will take their place.\n",
      "Sentence sentiment score: -0.5\n",
      "Sentence sentiment magnitude: 0.5\n",
      "Sentence text: Show Less\n",
      "Advice to Management\n",
      "Please continue to be transparent.\n",
      "Sentence sentiment score: -0.4000000059604645\n",
      "Sentence sentiment magnitude: 0.4000000059604645\n",
      "Sentence text: Please continue to help us make organization\n",
      "changes that lead to better outcomes for our clients and ourselves.\n",
      "Sentence sentiment score: 0.699999988079071\n",
      "Sentence sentiment magnitude: 0.699999988079071\n",
      "Sentence text: But mostly, please\n",
      "continue to invest in us the employees.\n",
      "Sentence sentiment score: -0.20000000298023224\n",
      "Sentence sentiment magnitude: 0.20000000298023224\n",
      "Sentence text: I've watched these folks make miracles happen\n",
      "for you to help you achieve your goals and dreams.\n",
      "Sentence sentiment score: 0.8999999761581421\n",
      "Sentence sentiment magnitude: 0.8999999761581421\n",
      "Sentence text: Putting your employees first is what\n",
      "allows us to be great - I sincerely and deeply hope that will always continue going\n",
      "forward.\n",
      "Sentence sentiment score: 0.800000011920929\n",
      "Sentence sentiment magnitude: 0.800000011920929\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "Sent_PDF('gs://buck910/Review1.pdf','gs://buck910/Review1_out.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extracting Sentiment from a  PDF in local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(annotations):\n",
    "    score = annotations.document_sentiment.score\n",
    "    magnitude = annotations.document_sentiment.magnitude\n",
    "\n",
    "    for index, sentence in enumerate(annotations.sentences):\n",
    "        sentence_sentiment = sentence.sentiment.score\n",
    "        print('Sentence {} has a sentiment score of {}'.format(\n",
    "            index, sentence_sentiment))\n",
    "    \n",
    "    print('Overall Sentiment: score of {} with magnitude of {}'.format(\n",
    "        score, magnitude))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def analyze(movie_review_filename):\n",
    "    from google.cloud.vision import enums\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    \n",
    "    mime_type = \"application/pdf\"\n",
    "    with io.open(movie_review_filename, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    input_config = {\"mime_type\": mime_type, \"content\": content}\n",
    "    features = [{\"type\": enums.Feature.Type.DOCUMENT_TEXT_DETECTION}]\n",
    "\n",
    "    pages = [1, 2, -1]\n",
    "    requests = [{\"input_config\": input_config, \"features\": features, \"pages\": pages}]\n",
    "\n",
    "    from google.cloud.language import enums\n",
    "    response = client.batch_annotate_files(requests)\n",
    "    for image_response in response.responses[0].responses:\n",
    "        test=format(image_response.full_text_annotation.text)\n",
    "    content = test.replace('\\n','')\n",
    "   \n",
    "    client = language.LanguageServiceClient()\n",
    "    document = types.Document(\n",
    "        content=content,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    annotations = client.analyze_sentiment(document=document)\n",
    "    \n",
    "    # Print the results\n",
    "    print(document)\n",
    "    print_result(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: PLAIN_TEXT\n",
      "content: \"What this book is aboutNeural networks are one of the most beautiful programming paradigms ever invented. In the conventional approach to programming, we tell the computer what to do, breaking big problems up into many small, precisely defined tasks that the computer can easily perform. By contrast, in a neural network we don\\342\\200\\231t tell the computer how to solve our problem. Instead, it learns from observational data, figuring out its own solution to the problem at hand. Automatically learning from data sounds promising. However, until 2006 we didn\\342\\200\\231t know how to train neural networks to surpass more traditional approaches, except for a few specialized problems. What changed in 2006 was the discovery of techniques for learning in so-called deep neural networks. These techniques are now known as deep learning. They\\342\\200\\231ve been developed further, and today deep neural networks and deep learning achieve outstanding performance on many important problems in computer vision, speech recognition, and natural language processing. They\\342\\200\\231re being deployed on a large scale by companies such as Google, Microsoft, and Facebook. The purpose of this book is to help you master the core concepts of neural networks, including modern techniques for deep learning. After working through the book you will have written code that uses neural networks and deep learning to solve complex pattern recognition problems. And you will have a foundation to use neural networks and deep learning to attack problems of your own devising.A principle-oriented approachOne conviction underlying the book is that it\\342\\200\\231s better to obtain a solid understanding of the core principles of neural networks and deep learning, rather than a hazy understanding of a long laundry list of ideas. If you\\342\\200\\231ve understood the core ideas well, you can rapidly understand other new material. In programming language terms, think of it as mastering the core syntax, libraries and data structures of a new language. You may still only \\342\\200\\234know\\342\\200\\235 a tiny fraction of the total language \\342\\200\\223 many languages have enormous standard libraries \\342\\200\\223 but new libraries and data structures can be understood quickly and easily. This means the book is emphatically not a tutorial in how to use some particular neural network library. If you mostly want to learn your way around a library, don\\342\\200\\231t read this book! Find the library you wish to learn, and work through the tutorials and documentation. But be warned. While this has an immediate problem-solving payoff, if you want to understand what\\342\\200\\231s really going on in neural networks, if you want insights that will still be relevant years from now, then it\\342\\200\\231s not enough just to learn some hot library. You need to understand the durable, lasting insights underlying how neural networks work. Technologies come and technologies go, but insight is forever.\"\n",
      "\n",
      "Sentence 0 has a sentiment score of 0.8999999761581421\n",
      "Sentence 1 has a sentiment score of 0.10000000149011612\n",
      "Sentence 2 has a sentiment score of -0.6000000238418579\n",
      "Sentence 3 has a sentiment score of -0.10000000149011612\n",
      "Sentence 4 has a sentiment score of 0.6000000238418579\n",
      "Sentence 5 has a sentiment score of -0.10000000149011612\n",
      "Sentence 6 has a sentiment score of 0.0\n",
      "Sentence 7 has a sentiment score of 0.699999988079071\n",
      "Sentence 8 has a sentiment score of 0.8999999761581421\n",
      "Sentence 9 has a sentiment score of 0.0\n",
      "Sentence 10 has a sentiment score of 0.5\n",
      "Sentence 11 has a sentiment score of 0.20000000298023224\n",
      "Sentence 12 has a sentiment score of 0.5\n",
      "Sentence 13 has a sentiment score of 0.699999988079071\n",
      "Sentence 14 has a sentiment score of 0.10000000149011612\n",
      "Sentence 15 has a sentiment score of 0.6000000238418579\n",
      "Sentence 16 has a sentiment score of -0.699999988079071\n",
      "Sentence 17 has a sentiment score of 0.0\n",
      "Sentence 18 has a sentiment score of 0.5\n",
      "Sentence 19 has a sentiment score of -0.4000000059604645\n",
      "Sentence 20 has a sentiment score of 0.0\n",
      "Sentence 21 has a sentiment score of -0.10000000149011612\n",
      "Sentence 22 has a sentiment score of 0.20000000298023224\n",
      "Overall Sentiment: score of 0.20000000298023224 with magnitude of 9.5\n"
     ]
    }
   ],
   "source": [
    "analyze('C:\\\\Users\\\\ajay\\\\Documents\\\\nlp\\\\SampleTest.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
